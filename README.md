### Hi there üëã

I'm St√©phan Tulkens! I'm a computational linguistics/AI person. I am currently working as a data scientist at [dataprovider.com](https://www.dataprovider.com), where I mainly work on small classifiers that operate on large volumes of web text. I live in beautiful Groningen, together with my partner and twin boy and girl. 

I got my Phd at [CLiPS](https://www.uantwerpen.be/en/research-groups/clips/) at the University of Antwerpen under the watchful eyes of Walter Daelemans (Computational Linguistics) and Dominiek Sandra (Psycholinguistics). The topic of my Phd was the way people process orthography during reading. You can find a copy [here](https://scholar.google.com/scholar?oi=bibs&hl=en&cluster=11519863597548395702).
Before that I studied computational linguistics (Ma), philosophy (Ba) and software engineering (Ba)

My goal is always to make things as fast and small as possible. I like it when simple models work well, and I love it when simple models get close in accuracy to big models. I do not believe absolute accuracy is a metric to be chased, and I think we should always be mindful of what a model computes or learns from the data.

#### I‚Äôm currently working on üèÉ‚Äç‚ôÇÔ∏è:
* [reach](https://github.com/stephantul/reach): a library for loading and working with word embeddings.
* [piecelearn](https://github.com/stephantul/piecelearn): a library that trains a subword tokenizer and embeddings on the same corpus, giving you open vocabulary embeddings.
* [unitoken](https://github.com/stephantul/unitoken): a library for easy pre-tokenization.

#### My research interests ü§ñ:
* Tokenizers, specifically subword tokenizers.
* Embeddings, specifically _static_ embeddings (so old-fashioned! üíÄ), and how to combine these in meaningful ways.
* String similarity, and how to compute it without using dynamic programming.

#### Contact:
* [My website](https://stephantul.github.io)
* [Google Scholar](https://scholar.google.com/citations?user=pvoqmHQAAAAJ)
* [Twitter](https://twitter.com/tulkenss)

